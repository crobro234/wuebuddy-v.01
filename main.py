from fastapi import FastAPI, HTTPException, Form
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
import sqlite3
import os
from openai import OpenAI
import numpy as np
from pydantic import BaseModel
from typing import List, Dict

# ===============================
#  ì´ˆê¸° ì„¤ì •
# ===============================
DB_PATH = "data/exchange_helper.db"
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===============================
#  HTML (index.html) ë¼ìš°íŠ¸
# ===============================
@app.get("/", response_class=HTMLResponse)
def index():
    with open("index.html", encoding="utf-8") as f:
        return f.read()

# ===============================
#  DB ê´€ë ¨ í•¨ìˆ˜
# ===============================
def get_all_questions():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT id, question, answer FROM questions")
    rows = cur.fetchall()
    conn.close()
    return [{"id": r[0], "question": r[1], "answer": r[2]} for r in rows]

# ===============================
#  ê¸°ì¡´ API (ê·¸ëŒ€ë¡œ ìœ ì§€)
# ===============================
@app.get("/categories")
def get_categories():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT * FROM categories")
    data = [{"id": row[0], "name": row[1]} for row in cur.fetchall()]
    conn.close()
    return {"categories": data}

@app.get("/questions/{category_id}")
def get_questions(category_id: int):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT id, question FROM questions WHERE category_id=?", (category_id,))
    data = [{"id": row[0], "question": row[1]} for row in cur.fetchall()]
    conn.close()
    return {"questions": data}

@app.get("/answer/{question_id}")
def get_answer(question_id: int):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT answer FROM questions WHERE id=?", (question_id,))
    row = cur.fetchone()
    conn.close()
    if not row:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return {"answer": row[0]}

# ===============================
#  ğŸ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
# ===============================
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

@app.get("/search")
def semantic_search(query: str):
    """OpenAI ì„ë² ë”© ê¸°ë°˜ ë¬¸ë§¥ ê²€ìƒ‰"""
    try:
        questions = get_all_questions()
        if not questions:
            return {"results": []}

        query_embed = client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        ).data[0].embedding

        scored = []
        for q in questions:
            q_embed = client.embeddings.create(
                model="text-embedding-3-small",
                input=q["question"]
            ).data[0].embedding
            similarity = cosine_similarity(np.array(query_embed), np.array(q_embed))
            scored.append((q, similarity))

        threshold = 0.3
        results = [
            item[0] for item in sorted(scored, key=lambda x: x[1], reverse=True)
            if item[1] >= threshold
        ]
        return {"results": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ===============================
#  ğŸ¤– AI ì±—ë´‡ ê¸°ëŠ¥
# ===============================
def embed_text(text: str) -> List[float]:
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

def top_k_qa_context(query: str, k: int = 3) -> List[Dict[str, str]]:
    """ë¡œì»¬ DBì˜ Q/A ì¤‘ ì§ˆì˜ì™€ ê°€ì¥ ê°€ê¹Œìš´ kê°œ ë°˜í™˜"""
    questions = get_all_questions()
    if not questions:
        return []
    q_embed = np.array(embed_text(query))
    scored = []
    for item in questions:
        e = np.array(embed_text(item["question"]))
        sim = cosine_similarity(q_embed, e)
        scored.append((item, sim))
    scored.sort(key=lambda x: x[1], reverse=True)
    return [x[0] for x in scored[:k] if x[1] >= 0.25]

class ChatRequest(BaseModel):
    message: str

@app.post("/chat")
def chat(req: ChatRequest):
    """AI ì±—ë´‡: ë¡œì»¬ Q/A ë¬¸ë§¥ì„ ì°¸ê³ í•´ ëŒ€í™”í˜• ì‘ë‹µ ì œê³µ"""
    try:
        ctx_items = top_k_qa_context(req.message, k=3)
        ctx_text = "\n\n".join([f"- Q: {x['question']}\n  A: {x['answer']}" for x in ctx_items]) or "ë¡œì»¬ ë¬¸ë§¥ ì—†ìŒ"

        system_prompt = (
            "ë‹¹ì‹ ì€ ë…ì¼ ë·”ë¥´ì¸ ë¶€ë¥´í¬ êµí™˜í•™ìƒ ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤. "
            "ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì´ê³  ë‹¨ê³„ë³„ë¡œ ë‹µí•˜ì„¸ìš”. "
            "ë¡œì»¬ ë¬¸ë§¥(ì•„ë˜ ì œê³µ)ê³¼ ìƒì¶©ë˜ë©´ ë¡œì»¬ ë¬¸ë§¥ì„ ìš°ì„ í•˜ì„¸ìš”. "
            "í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  'í™•ì¸ í•„ìš”'ë¼ê³  ë§í•˜ì„¸ìš”."
        )

        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.4,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user",
                 "content": f"ì‚¬ìš©ì ì§ˆë¬¸:\n{req.message}\n\n[ë¡œì»¬ ë¬¸ë§¥]\n{ctx_text}"}
            ]
        )

        answer = completion.choices[0].message.content.strip()
        return {"answer": answer, "context": ctx_items}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat error: {e}")

# ===============================
#  ì‹¤í–‰ (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
# ===============================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=10000)
